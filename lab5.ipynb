{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4beeb8f",
   "metadata": {},
   "source": [
    "Name:Laxmi Saud\n",
    "CRN:19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c68a0c7",
   "metadata": {},
   "source": [
    "Lab 5: Web scrabing using python\n",
    "\n",
    "Aim\n",
    "To understand the basic of web scraping using pthon libraries such as requests and beautiful soup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa621591",
   "metadata": {},
   "source": [
    " Question 1: Basic HTML Request & Parsing\n",
    "\n",
    "Theory\n",
    "Web scraping starts with sending an HTTP request to a webpage and retrieving its HTML content. The **requests** library is used to fetch web pages, while **BeautifulSoup** helps parse and navigate HTML documents. Handling exceptions is important to ensure the program does not crash due to network or server issues.Web scraping is the process of automatically extracting data from websites.\n",
    "Instead of manually copying and pasting, a program fetches the HTML content of a webpage and extracts the required information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e820200a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page Title: GeeksforGeeks | Your All-in-One Learning Portal\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "try:\n",
    "    response = requests.get(\"https://www.geeksforgeeks.org\", timeout=10)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    print(\"Page Title:\", soup.title.text)\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce8892e",
   "metadata": {},
   "source": [
    "Question 2:Extract Links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59c36cd",
   "metadata": {},
   "source": [
    " Theory\n",
    "Hyperlinks are represented using `<a>` tags in HTML. BeautifulSoup provides `.find()` and `.find_all()` methods to extract elements from a webpage. This helps in collecting URLs and anchor text for navigation or data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed0ced38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> https://www.geeksforgeeks.org/\n",
      "DSA -> https://www.geeksforgeeks.org/dsa/dsa-tutorial-learn-data-structures-and-algorithms/\n",
      "Practice Problems -> https://www.geeksforgeeks.org/explore\n",
      "C -> https://www.geeksforgeeks.org/c/c-programming-language/\n",
      "C++ -> https://www.geeksforgeeks.org/cpp/c-plus-plus/\n"
     ]
    }
   ],
   "source": [
    "links = soup.find_all('a', limit=5)\n",
    "for link in links:\n",
    "    print(link.get_text(strip=True), \"->\", link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361664bd",
   "metadata": {},
   "source": [
    " Question 3: Extract Headings\n",
    "\n",
    " Theory\n",
    "HTML headings such as `<h2>` represent section titles. Scraping headings is useful for content summarization and indexing. Extracted data can be stored into files like CSV for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c296a17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headings saved to headings.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "headings = [h.text.strip() for h in soup.find_all('h2')]\n",
    "\n",
    "with open('headings.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for h in headings:\n",
    "        writer.writerow([h])\n",
    "\n",
    "print(\"Headings saved to headings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e175f6",
   "metadata": {},
   "source": [
    "Question 4: Scrape Wikipedia Table\n",
    "\n",
    "Theory\n",
    "Tables in HTML consist of `<table>`, `<tr>`, and `<td>` tags. Scraping tables allows structured data extraction. Proper encoding and exception handling ensures reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb50f886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 403 Client Error: Forbidden for url: https://en.wikipedia.org/wiki/List_of_countries_by_population\n"
     ]
    }
   ],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/List_of_countries_by_population\"\n",
    "\n",
    "try:\n",
    "    res = requests.get(url, timeout=10)\n",
    "    res.raise_for_status()\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    table = soup.find('table', {'class': 'wikitable'})\n",
    "    \n",
    "    for row in table.find_all('tr'):\n",
    "        cells = [cell.text.strip() for cell in row.find_all(['td','th'])]\n",
    "        print(cells)\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f66fa22",
   "metadata": {},
   "source": [
    "## Question 5: Selectors and Navigation\n",
    "\n",
    "### Theory\n",
    "BeautifulSoup supports DOM navigation such as parents, siblings, and attribute-based selection. This enables fine-grained traversal of HTML elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "617b6b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<p class=\"intro\">Welcome</p>, <p class=\"intro\">Learn Python</p>]\n",
      "Parent of <a>: <body>\n",
      "<p class=\"intro\">Welcome</p>\n",
      "<p class=\"intro\">Learn Python</p>\n",
      "<a href=\"https://python.org\">Python</a>\n",
      "</body>\n",
      "Next sibling: <p class=\"intro\">Learn Python</p>\n"
     ]
    }
   ],
   "source": [
    "html = '''<html><body>\n",
    "<p class=\"intro\">Welcome</p>\n",
    "<p class=\"intro\">Learn Python</p>\n",
    "<a href=\"https://python.org\">Python</a>\n",
    "</body></html>'''\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "print(soup.find_all('p', class_='intro'))\n",
    "print(\"Parent of <a>:\", soup.find('a').parent)\n",
    "print(\"Next sibling:\", soup.find('p').find_next_sibling())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4c3514",
   "metadata": {},
   "source": [
    "Question 6: Tag Manipulation\n",
    "\n",
    " Theory\n",
    "BeautifulSoup allows modification of HTML tags such as changing tag names, adding attributes, and updating text dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c3d3bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<strong class=\"boldest\" id=\"greeting\">Hi there</strong>\n"
     ]
    }
   ],
   "source": [
    "tag = BeautifulSoup('<b class=\"boldest\">Hello</b>', 'html.parser').b\n",
    "tag.name = 'strong'\n",
    "tag['id'] = 'greeting'\n",
    "tag.string = 'Hi there'\n",
    "print(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12e3f0a",
   "metadata": {},
   "source": [
    "## Question 7: Advanced Navigation\n",
    "\n",
    "### Theory\n",
    "Advanced navigation techniques help locate text nodes and their related elements such as parents and siblings within an HTML structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b043d2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent td: <td>Apple</td>\n",
      "Siblings: []\n"
     ]
    }
   ],
   "source": [
    "html = '''<table><tr><td>Apple</td></tr><tr><td>Banana</td></tr></table>'''\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "apple = soup.find(string='Apple')\n",
    "print(\"Parent td:\", apple.parent)\n",
    "print(\"Siblings:\", apple.parent.find_next_siblings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0f5d40",
   "metadata": {},
   "source": [
    "## Question 8: Using SoupStrainer\n",
    "\n",
    "### Theory\n",
    "SoupStrainer improves efficiency by parsing only specific tags instead of the entire document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ac79a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"page1.html\">Page 1</a><a href=\"page2.html\">Page 2</a>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import SoupStrainer\n",
    "\n",
    "html = '''<html><a href=\"page1.html\">Page 1</a><p>Paragraph</p><a href=\"page2.html\">Page 2</a></html>'''\n",
    "only_a = SoupStrainer('a')\n",
    "soup = BeautifulSoup(html, 'html.parser', parse_only=only_a)\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8cf481",
   "metadata": {},
   "source": [
    " Question 9: Exception Handling\n",
    "\n",
    " Theory\n",
    "Robust web scraping requires handling exceptions such as timeouts, HTTP errors, missing elements, and request failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df1dc27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Error\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = requests.get(url, timeout=5)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    table = soup.find('table')\n",
    "    if table is None:\n",
    "        raise AttributeError('Table not found')\n",
    "except requests.exceptions.Timeout:\n",
    "    print('Timeout Error')\n",
    "except requests.exceptions.HTTPError:\n",
    "    print('HTTP Error')\n",
    "except requests.exceptions.RequestException:\n",
    "    print('Request Error')\n",
    "except AttributeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7d8274",
   "metadata": {},
   "source": [
    "Discussion\n",
    "This lab demonstrated how Python can be effectively used for web scraping tasks. Students learned how to fetch web pages, parse HTML, extract structured and unstructured data, navigate DOM trees, manipulate tags, and handle exceptions gracefully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d82130",
   "metadata": {},
   "source": [
    " Conclusion\n",
    "Web scraping is a powerful technique for data collection from the internet. By using requests and BeautifulSoup, complex HTML documents can be parsed efficiently. Proper exception handling and ethical scraping practices ensure reliability and maintainability of scraping programs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
